checking model...
CausalLMOutputWithPast(loss=None, logits=tensor([[[-10.2661,  -5.5258,  -7.3737,  ...,  -5.8334,  -6.2495,  -4.6525],
         [ -3.4294,  -3.7293,  -4.7117,  ...,   1.2367,  -1.0389,  -2.5274],
         [ -5.4232,  -4.4565,  -2.5109,  ...,  -1.9982,  -4.1886,  -4.5079],
         ...,
         [  0.9243,  -1.8582,   1.2435,  ...,   1.5970,  -1.2174,  -1.3044],
         [ -8.0615,  -9.8660,  -5.1835,  ...,  -6.2493,  -9.4979,  -8.6617],
         [ -0.6751,   2.8443,   0.6372,  ...,   0.4437,  -1.4547,  -0.7482]]],
       grad_fn=<UnsafeViewBackward0>), past_key_values=DynamicCache(layers=[DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer]), hidden_states=None, attentions=None)
mock training model (to output zeros)...
param count: (39936, {'ehlg': (39936, {'emb': 39936, 'o2l': 0, 'model': 0}), 'olm': 0, 'total': 39936})
Step     1 of   100 complete...
loss: 35.786529541015625
loss: 6.719855308532715
loss: 5.279665946960449
loss: 11.831311225891113
loss: 5.081218719482422
loss: 8.563933372497559
loss: 5.540005683898926
loss: 5.325274467468262
loss: 7.25654411315918
loss: 5.211629390716553
Step    11 of   100 complete...
loss: 5.657443523406982
loss: 5.0868635177612305
loss: 7.255589485168457
loss: 5.4334306716918945
loss: 7.87052583694458
loss: 4.235953330993652
loss: 6.446381092071533
loss: 9.875564575195312
loss: 5.474610328674316
loss: 3.5475168228149414
Step    21 of   100 complete...
loss: 10.84441089630127
loss: 12.684667587280273
loss: 5.916345596313477
loss: 11.006946563720703
loss: 6.269969463348389
loss: 7.954941749572754
loss: 7.328604221343994
loss: 5.666719913482666
loss: 9.306427955627441
loss: 8.841779708862305
Step    31 of   100 complete...
loss: 9.090409278869629
loss: 5.635018825531006
loss: 6.970608234405518
loss: 4.612931728363037
loss: 7.69260835647583
loss: 6.829463005065918
loss: 8.922272682189941
loss: 4.03904914855957
loss: 10.907319068908691
loss: 4.767398357391357
Step    41 of   100 complete...
loss: 10.073019981384277
loss: 6.883723258972168
loss: 8.266940116882324
loss: 7.369618892669678
loss: 3.8253750801086426
loss: 6.693697452545166
loss: 6.4876813888549805
loss: 6.5006794929504395
loss: 7.992884159088135
loss: 5.484249114990234
Step    51 of   100 complete...
loss: 9.339635848999023
loss: 7.516989231109619
loss: 4.363055229187012
loss: 6.435725212097168
loss: 7.918981075286865
loss: 5.423103332519531
loss: 9.251282691955566
loss: 6.196105003356934
loss: 6.388600826263428
loss: 11.115435600280762
Step    61 of   100 complete...
loss: 9.24669075012207
loss: 8.01664924621582
loss: 7.57781982421875
loss: 5.94319486618042
loss: 5.951506614685059
loss: 8.131956100463867
loss: 13.844732284545898
loss: 9.102869033813477
loss: 9.878473281860352
loss: 13.265623092651367
Step    71 of   100 complete...
loss: 9.993881225585938
loss: 6.558112621307373
loss: 9.583099365234375
loss: 7.472259521484375
loss: 4.422955513000488
loss: 6.439525604248047
loss: 8.360123634338379
loss: 8.244614601135254
loss: 10.670632362365723
loss: 5.74174165725708
Step    81 of   100 complete...
loss: 7.431900501251221
loss: 4.522599220275879
loss: 6.6378560066223145
loss: 6.062336444854736
loss: 11.8108549118042
loss: 10.62905502319336
loss: 6.773238658905029
loss: 4.930413722991943
loss: 8.010682106018066
loss: 6.33957052230835
Step    91 of   100 complete...
loss: 10.644594192504883
loss: 9.085555076599121
loss: 10.914522171020508
loss: 6.32185697555542
loss: 5.28541898727417
loss: 7.067867279052734
loss: 5.589927673339844
loss: 5.866308689117432
loss: 6.926068305969238
loss: 5.7380900382995605
Step   100 of   100 complete...
checking mock trained model...
CausalLMOutputWithPast(loss=None, logits=tensor([[[ 3.7132,  5.1292,  0.7137,  ...,  3.4142,  3.5731, -0.3197],
         [-0.6903, -1.0690,  0.4046,  ..., -3.4847, -1.7156, -3.9493],
         [-2.0453, -1.0685, -0.8381,  ..., -4.8367, -2.3932, -2.1840],
         ...,
         [ 0.2784,  2.9655, -2.0562,  ..., -2.6837, -0.8015, -1.9089],
         [-5.3070, -1.3482,  0.0807,  ..., -4.0953,  0.9395, -1.7369],
         [-1.6281, -3.8556, -0.0336,  ...,  1.6676,  1.7409, -0.7523]]],
       grad_fn=<UnsafeViewBackward0>), past_key_values=DynamicCache(layers=[DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer]), hidden_states=None, attentions=None)