EHLGOLM(
  (olm): OpenLLaMAv2Model(
    (model): LlamaForCausalLM(
      (model): LlamaModel(
        (embed_tokens): Embedding(32000, 3200, padding_idx=0)
        (layers): ModuleList(
          (0-25): 26 x LlamaDecoderLayer(
            (self_attn): LlamaAttention(
              (q_proj): Linear(in_features=3200, out_features=3200, bias=False)
              (k_proj): LinearLoRA(
                (linear): Linear(in_features=3200, out_features=3200, bias=False)
              )
              (v_proj): LinearLoRA(
                (linear): Linear(in_features=3200, out_features=3200, bias=False)
              )
              (o_proj): Linear(in_features=3200, out_features=3200, bias=False)
            )
            (mlp): LlamaMLP(
              (gate_proj): Linear(in_features=3200, out_features=8640, bias=False)
              (up_proj): Linear(in_features=3200, out_features=8640, bias=False)
              (down_proj): Linear(in_features=8640, out_features=3200, bias=False)
              (act_fn): SiLU()
            )
            (input_layernorm): LlamaRMSNorm((3200,), eps=1e-06)
            (post_attention_layernorm): LlamaRMSNorm((3200,), eps=1e-06)
          )
        )
        (norm): LlamaRMSNorm((3200,), eps=1e-06)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (lm_head): Linear(in_features=3200, out_features=32000, bias=False)
    )
    (llama): LlamaForCausalLM(
      (model): LlamaModel(
        (embed_tokens): Embedding(32000, 3200, padding_idx=0)
        (layers): ModuleList(
          (0-25): 26 x LlamaDecoderLayer(
            (self_attn): LlamaAttention(
              (q_proj): Linear(in_features=3200, out_features=3200, bias=False)
              (k_proj): LinearLoRA(
                (linear): Linear(in_features=3200, out_features=3200, bias=False)
              )
              (v_proj): LinearLoRA(
                (linear): Linear(in_features=3200, out_features=3200, bias=False)
              )
              (o_proj): Linear(in_features=3200, out_features=3200, bias=False)
            )
            (mlp): LlamaMLP(
              (gate_proj): Linear(in_features=3200, out_features=8640, bias=False)
              (up_proj): Linear(in_features=3200, out_features=8640, bias=False)
              (down_proj): Linear(in_features=8640, out_features=3200, bias=False)
              (act_fn): SiLU()
            )
            (input_layernorm): LlamaRMSNorm((3200,), eps=1e-06)
            (post_attention_layernorm): LlamaRMSNorm((3200,), eps=1e-06)
          )
        )
        (norm): LlamaRMSNorm((3200,), eps=1e-06)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (lm_head): Linear(in_features=3200, out_features=32000, bias=False)
    )
    (llamav2): LlamaForCausalLM(
      (model): LlamaModel(
        (embed_tokens): Embedding(32000, 3200, padding_idx=0)
        (layers): ModuleList(
          (0-25): 26 x LlamaDecoderLayer(
            (self_attn): LlamaAttention(
              (q_proj): Linear(in_features=3200, out_features=3200, bias=False)
              (k_proj): LinearLoRA(
                (linear): Linear(in_features=3200, out_features=3200, bias=False)
              )
              (v_proj): LinearLoRA(
                (linear): Linear(in_features=3200, out_features=3200, bias=False)
              )
              (o_proj): Linear(in_features=3200, out_features=3200, bias=False)
            )
            (mlp): LlamaMLP(
              (gate_proj): Linear(in_features=3200, out_features=8640, bias=False)
              (up_proj): Linear(in_features=3200, out_features=8640, bias=False)
              (down_proj): Linear(in_features=8640, out_features=3200, bias=False)
              (act_fn): SiLU()
            )
            (input_layernorm): LlamaRMSNorm((3200,), eps=1e-06)
            (post_attention_layernorm): LlamaRMSNorm((3200,), eps=1e-06)
          )
        )
        (norm): LlamaRMSNorm((3200,), eps=1e-06)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (lm_head): Linear(in_features=3200, out_features=32000, bias=False)
    )
  )
  (ehlg): EMBGPT2LoRAGen(
    (emb): Embedding(52, 768)
    (gpt2): GPT2LMHeadModel(
      (transformer): GPT2Model(
        (wte): Embedding(50257, 768)
        (wpe): Embedding(1024, 768)
        (drop): Dropout(p=0.1, inplace=False)
        (h): ModuleList(
          (0-11): 12 x GPT2Block(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): GPT2Attention(
              (c_attn): Conv1D(nf=2304, nx=768)
              (c_proj): Conv1D(nf=768, nx=768)
              (attn_dropout): Dropout(p=0.1, inplace=False)
              (resid_dropout): Dropout(p=0.1, inplace=False)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): GPT2MLP(
              (c_fc): Conv1D(nf=3072, nx=768)
              (c_proj): Conv1D(nf=768, nx=3072)
              (act): NewGELUActivation()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (lm_head): Linear(in_features=768, out_features=50257, bias=False)
    )
    (hypernetwork): GPT2LMHeadModel(
      (transformer): GPT2Model(
        (wte): Embedding(50257, 768)
        (wpe): Embedding(1024, 768)
        (drop): Dropout(p=0.1, inplace=False)
        (h): ModuleList(
          (0-11): 12 x GPT2Block(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): GPT2Attention(
              (c_attn): Conv1D(nf=2304, nx=768)
              (c_proj): Conv1D(nf=768, nx=768)
              (attn_dropout): Dropout(p=0.1, inplace=False)
              (resid_dropout): Dropout(p=0.1, inplace=False)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): GPT2MLP(
              (c_fc): Conv1D(nf=3072, nx=768)
              (c_proj): Conv1D(nf=768, nx=3072)
              (act): NewGELUActivation()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (lm_head): Linear(in_features=768, out_features=50257, bias=False)
    )
    (inner_model): GPT2LMHeadModel(
      (transformer): GPT2Model(
        (wte): Embedding(50257, 768)
        (wpe): Embedding(1024, 768)
        (drop): Dropout(p=0.1, inplace=False)
        (h): ModuleList(
          (0-11): 12 x GPT2Block(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): GPT2Attention(
              (c_attn): Conv1D(nf=2304, nx=768)
              (c_proj): Conv1D(nf=768, nx=768)
              (attn_dropout): Dropout(p=0.1, inplace=False)
              (resid_dropout): Dropout(p=0.1, inplace=False)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): GPT2MLP(
              (c_fc): Conv1D(nf=3072, nx=768)
              (c_proj): Conv1D(nf=768, nx=3072)
              (act): NewGELUActivation()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (lm_head): Linear(in_features=768, out_features=50257, bias=False)
    )
    (model): GPT2LMHeadModel(
      (transformer): GPT2Model(
        (wte): Embedding(50257, 768)
        (wpe): Embedding(1024, 768)
        (drop): Dropout(p=0.1, inplace=False)
        (h): ModuleList(
          (0-11): 12 x GPT2Block(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): GPT2Attention(
              (c_attn): Conv1D(nf=2304, nx=768)
              (c_proj): Conv1D(nf=768, nx=768)
              (attn_dropout): Dropout(p=0.1, inplace=False)
              (resid_dropout): Dropout(p=0.1, inplace=False)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): GPT2MLP(
              (c_fc): Conv1D(nf=3072, nx=768)
              (c_proj): Conv1D(nf=768, nx=3072)
              (act): NewGELUActivation()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (lm_head): Linear(in_features=768, out_features=50257, bias=False)
    )
    (o2l): Linear(in_features=768, out_features=51200, bias=True)
  )
)