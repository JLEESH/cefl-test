checking model...
<s>This is a sample sentence.aneanandedengondon interferenceStock Associationrays“ê Tur netilon

checking model (unadapted output)...
<s>This is a sample sentence.
This is a sample sentence.
This is a sample

mock training model...
param count: (39412736, {'ehlg': (39412736, {'emb': 39936, 'o2l': 39372800, 'model': 0}), 'olm': 0, 'total': 39412736})
Step     1 of   100 complete...
loss: 10.373517990112305
loss: 10.373489379882812
loss: 10.37345027923584
loss: 10.373526573181152
loss: 10.37352466583252
loss: 10.373510360717773
loss: 10.373498916625977
loss: 10.373528480529785
loss: 10.373517036437988
loss: 10.373488426208496
Step    11 of   100 complete...
loss: 10.37348461151123
loss: 10.373494148254395
(39412736, {'ehlg': (39412736, {'emb': 39936, 'o2l': 39372800, 'model': 0}), 'olm': 0, 'total': 39412736})
loss: 10.373505592346191
loss: 10.373512268066406
loss: 10.373486518859863
loss: 10.373478889465332
loss: 10.373526573181152
loss: 10.373480796813965
loss: 10.373489379882812
loss: 10.373504638671875
Step    21 of   100 complete...
loss: 10.373492240905762
loss: 10.373520851135254
loss: 10.373504638671875
loss: 10.373497009277344
loss: 10.373486518859863
loss: 10.373506546020508
loss: 10.373461723327637
loss: 10.373498916625977
loss: 10.373417854309082
loss: 10.373469352722168
Step    31 of   100 complete...
loss: 10.373501777648926
loss: 10.373476028442383
loss: 10.373504638671875
loss: 10.373475074768066
loss: 10.37345027923584
loss: 10.373490333557129
loss: 10.37347412109375
loss: 10.373468399047852
loss: 10.373499870300293
loss: 10.373502731323242
Step    41 of   100 complete...
loss: 10.373475074768066
loss: 10.373504638671875
loss: 10.373480796813965
loss: 10.37348461151123
loss: 10.373466491699219
loss: 10.373498916625977
loss: 10.373466491699219
loss: 10.37349796295166
loss: 10.373518943786621
loss: 10.373478889465332
Step    51 of   100 complete...
loss: 10.373486518859863
loss: 10.373482704162598
loss: 10.373490333557129
loss: 10.373465538024902
loss: 10.373421669006348
loss: 10.373469352722168
loss: 10.373434066772461
loss: 10.373491287231445
loss: 10.373420715332031
loss: 10.373486518859863
Step    61 of   100 complete...
loss: 10.373478889465332
loss: 10.373433113098145
loss: 10.373472213745117
loss: 10.373504638671875
loss: 10.373472213745117
loss: 10.373479843139648
loss: 10.373465538024902
loss: 10.37347412109375
loss: 10.373465538024902
loss: 10.373494148254395
Step    71 of   100 complete...
loss: 10.373497009277344
loss: 10.373480796813965
loss: 10.373496055603027
loss: 10.373489379882812
loss: 10.373517990112305
loss: 10.373473167419434
loss: 10.373469352722168
loss: 10.373225212097168
loss: 10.373512268066406
loss: 10.373446464538574
Step    81 of   100 complete...
loss: 10.373438835144043
loss: 10.37341594696045
loss: 10.373494148254395
loss: 10.373497009277344
loss: 10.373456954956055
loss: 10.373478889465332
loss: 10.373507499694824
loss: 10.37347412109375
loss: 10.373480796813965
loss: 10.373464584350586
Step    91 of   100 complete...
loss: 10.373414993286133
loss: 10.373459815979004
loss: 10.373506546020508
loss: 10.373446464538574
loss: 10.373489379882812
loss: 10.373469352722168
loss: 10.373496055603027
loss: 10.373458862304688
loss: 10.373461723327637
loss: 10.373507499694824
Step   100 of   100 complete...

checking mock trained model...
<s>This is a sample sentence.ancelANTY extÂè•ilst beyondÔøΩ tourn AntüîÖ‚ñº‡∏ø

checking mock trained model (unadapted output)...
<s>This is a sample sentence.
This is a sample sentence.
This is a sample